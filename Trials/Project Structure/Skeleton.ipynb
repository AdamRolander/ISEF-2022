{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import ipywidgets as widgets\n",
    "import io\n",
    "from PIL import Image\n",
    "from IPython.display import display,clear_output\n",
    "from warnings import filterwarnings\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "from glob import glob\n",
    "import keras\n",
    "from keras.applications.vgg16 import preprocess_input,VGG16\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import MaxPooling2D,Conv2D,Dense,BatchNormalization,Dropout,GlobalAveragePooling2D,Flatten,Input, Activation\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Connecting to Google Drive -- DATASET ACCESS\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing Dataset from Google Drive\n",
    "Training = '/content/gdrive/My Drive/Colab Notebooks/SET - Training'\n",
    "Testing = '/content/gdrive/My Drive/Colab Notebooks/SET - Testing'\n",
    "\n",
    "# Creating Tumor Class Labels\n",
    "labels = ['glioma', 'meningioma', 'notumor', 'pituitary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Training and Testing Arrays\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Image Size\n",
    "image_size = 224\n",
    "\n",
    "# Transferring Images from Google Drive to Arrays\n",
    "for i in labels:\n",
    "    folderPath = os.path.join(r'C:\\Users\\adamr\\Downloads\\archive (6)\\SET - Training', Training, i)\n",
    "    for j in tqdm(os.listdir(folderPath)):\n",
    "        img = cv2.imread(os.path.join(folderPath,j))\n",
    "        img = cv2.resize(img,(image_size, image_size))\n",
    "        X_train.append(img)\n",
    "        y_train.append(i)\n",
    "        \n",
    "for i in labels:\n",
    "    folderPath = os.path.join(r'C:\\Users\\adamr\\Downloads\\archive (6)\\SET - Testing', Testing, i)\n",
    "    for j in tqdm(os.listdir(folderPath)):\n",
    "        img = cv2.imread(os.path.join(folderPath,j))\n",
    "        img = cv2.resize(img,(image_size,image_size))\n",
    "        X_train.append(img)\n",
    "        y_train.append(i)\n",
    "        \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA AUGMENTATION\n",
    "# Rescaling used in all trials, including control\n",
    "# Rescale protocols are shown in other files!\n",
    "\n",
    "rescale = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling Images from Training and Testing Arrays\n",
    "X_train, y_train = shuffle(X_train,y_train, random_state = 101)\n",
    "\n",
    "# Splitting Arrays to 80% Training, 20% Testing\n",
    "# X_train and X_test contain the MRI images, y_train and y_test contain the labels for the respective images\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state = 101)\n",
    "\n",
    "# Applying the above rescales + augmentations\n",
    "rescale.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new y_train array with numerical labels for images\n",
    "y_train_new = []\n",
    "for i in y_train:\n",
    "    y_train_new.append(labels.index(i))\n",
    "y_train = y_train_new\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "\n",
    "# Creating new y_test array with numerical labels for images\n",
    "y_test_new = []\n",
    "for i in y_test:\n",
    "    y_test_new.append(labels.index(i))\n",
    "y_test = y_test_new\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CNN Model Architecture\n",
    "# Input Image Size \n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "## FEATURE LEARNING BLOCK ##\n",
    "# First Convolutional Layer -- Features: 128, Kernel Size: (3,3), Strides: (1,1), Padding: Valid, Activation Function: RELU, Batch Normalization\n",
    "model.add(Conv2D(128, (3, 3), input_shape=INPUT_SHAPE))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Second Convolutional Layer -- Features: 64, Kernel Size: (3,3), Strides: (1,1), Padding: Valid, Activation Function: RELU, Batch Normalization\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Third Convolutional Layer -- Features: 32, Kernel Size: (3,3), Strides: (1,1), Padding: Valid, Activation Function: RELU, Batch Normalization\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "##################################\n",
    "\n",
    "## CONCATONATED DENSE LAYERS BLOCK ##\n",
    "# Flattening Layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# First Dense Layer -- Activation Function: RELU, Dropout: 0.2\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Second Dense Layer -- Activation Function: RELU\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Third Dense Layer -- Activation Function: Softmax -- Returns Numerical Output (0-3) to Classification Block\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "##################################\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Model -- Epochs: 20, Steps Per Epoch: 69, Validation Split: 0.1, Batch Size: 32\n",
    "history = model.fit(X_train, y_train, steps_per_epoch = 69, validation_split=0.1, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure Color Hex Codes\n",
    "colors_dark = [\"#1F1F1F\", \"#313131\", '#636363', '#AEAEAE', '#DADADA']\n",
    "colors_red = [\"#331313\", \"#582626\", '#9E1717', '#D35151', '#E9B4B4']\n",
    "colors_green = ['#01411C','#4B6F44','#4F7942','#74C365','#D0F0C0']\n",
    "\n",
    "# Training Accuracy vs Validation Accuracy and Training Loss vs Validation Loss\n",
    "filterwarnings('ignore')\n",
    "\n",
    "epochs = [i for i in range(20)]\n",
    "fig, ax = plt.subplots(1,2,figsize=(14,7))\n",
    "train_acc = history.history['accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_acc = history.history['val_accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "fig.text(s='Epochs vs. Training and Validation Accuracy/Loss',size=18,fontweight='bold',\n",
    "             fontname='monospace',color=colors_dark[1],y=1,x=0.28,alpha=0.8)\n",
    "\n",
    "sns.despine()\n",
    "ax[0].plot(epochs, train_acc,color=colors_green[3],\n",
    "           label = 'Training Accuracy')\n",
    "ax[0].plot(epochs, val_acc,color=colors_red[3],\n",
    "           label = 'Validation Accuracy')\n",
    "ax[0].legend(frameon=False)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "\n",
    "sns.despine()\n",
    "ax[1].plot(epochs, train_loss, color=colors_green[3],\n",
    "           label ='Training Loss')\n",
    "ax[1].plot(epochs, val_loss, color=colors_red[3],\n",
    "           label = 'Validation Loss')\n",
    "ax[1].legend(frameon=False)\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Training & Validation Loss')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Evaluating CNN Model on Test Sets\n",
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_test_new = np.argmax(y_test,axis=1)\n",
    "\n",
    "# Returns Accuracy, Precision, Recall, F1 Score Report\n",
    "print(classification_report(y_test_new,pred))\n",
    "\n",
    "# Generating Confusion Matricx\n",
    "fig,ax=plt.subplots(1,1,figsize=(14,7))\n",
    "sns.heatmap(confusion_matrix(y_test_new,pred),ax=ax,xticklabels=labels,yticklabels=labels,annot=True,\n",
    "           cmap=colors_green[::-1],alpha=0.7,linewidths=2,linecolor=colors_dark[3])\n",
    "fig.text(s='Heatmap of the Confusion Matrix',size=18,fontweight='bold',\n",
    "             fontname='monospace',color=colors_dark[1],y=0.92,x=0.28,alpha=0.8)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
